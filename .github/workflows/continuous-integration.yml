name: Salesforce CI/CD Logic

# This workflow can ONLY be called from another repository
on:
  workflow_call:
    inputs:
      target-env:
        description: 'Salesforce Target Environment Alias (e.g., sandbox, prod)'
        required: true
        type: string
      deployment-mode:
        description: 'Deployment Mode (DELTA or FULL)'
        required: true
        type: string
      test-level:
        description: 'Test Level (NoTestRun, RunLocalTests, RunSpecifiedTests)'
        required: false
        type: string
      check-only:
        description: 'Run deployment as check-only (validation)'
        required: true
        type: boolean
      outputdir:
        description: 'Output directory for artifacts'
        required: false
        type: string
        default: 'results'
      change-detection-type:
        description: 'Metadata type for test class detection (e.g., classes, all)'
        required: false
        type: string
        default: 'classes'
      test-class-regex: 
        description: 'Regex for matching test class names'
        required: false
        type: string
        default: '.*Test.*$'
      org-preconfig:
        description: 'Run pre-deployment org configuration data step'
        required: false
        type: boolean
        default: false # Set a default value
      run-code-scanner:
        description: 'Run Salesforce code analyzer'
        required: false
        type: boolean
        default: false # Set a default value
    secrets: 
      AUTH_URL:
        required: true
jobs:
  salesforce_ci:
    runs-on: ubuntu-latest
    container: brovasi/dxb
    environment: ${{ inputs.target-env }}

    env:
      # Map inputs to environment variables used in the script
      TARGET_ENV: ${{ inputs.target-env }}
      DEPLOYMENT_MODE: ${{ inputs.deployment-mode }}
      CHECK_ONLY: ${{ inputs.check-only }}
      OUTPUT_DIR: ${{ inputs.outputdir }}
      CHANGE_DETECTION_TYPE: ${{ inputs.change-detection-type }}
      TEST_CLASS_REGEX: ${{ inputs.test-class-regex }}

    steps:
    
      - name: â¬‡ï¸ Checkout Repository (Salesforce Project)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: âš™ï¸ Initialize Pipeline
        run: |
          # Set COMPARE_WITH based on check-only
          COMPARE_WITH="${{ inputs.check-only == true && vars.VAL_COMPARE_WITH || vars.DEP_COMPARE_WITH }}"
          
          # Set DELTA_MODE based on check-only (using the ternary operator)
          DELTA_MODE="${{ inputs.check-only == true && vars.VAL_DELTA_MODE || vars.DEP_DELTA_MODE }}"
          
          # Set TEST_LEVEL using the nested condition
          TEST_LEVEL="${{ inputs.test-level != '' && inputs.test-level || (inputs.check-only == true && vars.VAL_TEST_LEVEL || vars.DEP_TEST_LEVEL) }}"
          
          # Export these variables so they are available to subsequent commands
          # (Optional, but good practice if you have more commands in this script)
          echo "COMPARE_WITH=$COMPARE_WITH" >> $GITHUB_ENV
          echo "DELTA_MODE=$DELTA_MODE" >> $GITHUB_ENV
          echo "TEST_LEVEL=$TEST_LEVEL" >> $GITHUB_ENV

          # -----------------------------------------------------------------
          # START: CREATE CONFIGURATION SUMMARY
          # -----------------------------------------------------------------
          
          echo "## âš™ï¸ CI/CD Configuration Summary" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "| Setting | Value |" >> $GITHUB_STEP_SUMMARY
          echo "| :--- | :--- |" >> $GITHUB_STEP_SUMMARY
          
          # Workflow Inputs
          echo "| **Target Environment** | ${{ inputs.target-env }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Deployment Mode | ${{ inputs.deployment-mode }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Delta Mode | $DELTA_MODE |" >> $GITHUB_STEP_SUMMARY
          echo "| Compare With | \`$COMPARE_WITH\` |" >> $GITHUB_STEP_SUMMARY
          echo "| **Check Only (Validation)** | ${{ inputs.check-only }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Test Level | $TEST_LEVEL |" >> $GITHUB_STEP_SUMMARY
          echo "| Run Code Scanner | ${{ inputs.run-code-scanner }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Run Org Pre-Config | ${{ inputs.org-preconfig }} |" >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Configuration established successfully. Starting pipeline execution..." >> $GITHUB_STEP_SUMMARY
          
          # -----------------------------------------------------------------
          # END: CREATE CONFIGURATION SUMMARY
          # -----------------------------------------------------------------
          
          npm install --save-dev
          sf plugins link $(npm root -g)/dxb
          sf plugins
          git config --global --add safe.directory $PWD
        shell: bash
          
      # 1. Login to Salesforce (using the common, fixed secret AUTH_URL)
      - name: ðŸ”‘ Salesforce Login
        run: |
          AUTH_URL_VALUE="${{ secrets.AUTH_URL }}"
          
          if [ -z "$AUTH_URL_VALUE" ]; then
            echo "::error::SFDX Auth URL secret not found. Ensure the calling workflow passes the secret via the 'SFDX_AUTH_URL' key."
            exit 1
          fi
          
          echo "$AUTH_URL_VALUE" > credentials.txt
          # Use the environment variable $TARGET_ENV
          sf org login sfdx-url -f credentials.txt -a $TARGET_ENV -s
        shell: bash
      # 2. Run Delta
      - name: ðŸ§® Calculate Delta
        id: calculate_delta
        run: |
          # Use environment variables which are easier to read and less error-prone
          if [ "$DEPLOYMENT_MODE" = "FULL" ]
          then
            echo "--- Full Deployment Mode ---"
            allPackageDirectories=""
            json=$(cat sfdx-project.json)
            for i in $(echo "$json" | jq -r '.packageDirectories[].path'); do
              allPackageDirectories+=" --source-dir $i"
            done
            echo "sf project generate manifest $allPackageDirectories -d $OUTPUT_DIR"
            sf project generate manifest $allPackageDirectories -d $OUTPUT_DIR
            
            # Setup for full deployment tests
            echo "./**/*.cls" > apexclasses
            echo "" > testClasses
            echo "::set-output name=noChanges::false" # Force deployment if FULL mode

          else # DELTA Deployment Mode
            echo "--- Delta Deployment Mode (Comparing with $COMPARE_WITH) ---"
            #increase git file name config
            #git config diff.renameLimit 999999
            
            # 1. Generate package.xml and check for changes
            echo "sf dxb source delta -m $DELTA_MODE -k $COMPARE_WITH -p $OUTPUT_DIR -g"
            sf dxb source delta -m $DELTA_MODE -k $COMPARE_WITH -p $OUTPUT_DIR -g
            
            # Check if package.xml contains actual metadata members
            if ! grep -q '<members>' "$OUTPUT_DIR/package.xml"; then
              echo "No changes detected. Skipping deployment."
              echo "noChanges=true" >> $GITHUB_OUTPUT
              exit 0
            fi

            # 2. Extract Apex classes and test classes for specified tests
            sf dxb source delta -k $COMPARE_WITH --json | jq -c '.result.deltaMeta[] | select (endswith(".cls"))' | paste -sd, - | sed 's/"//g' > apexclasses
            cat $OUTPUT_DIR/package.xml
            
            if [ "$TEST_LEVEL" = "RunSpecifiedTests" ]; then 
              echo "sf dxb source fetchtest -x $OUTPUT_DIR/package.xml -t $CHANGE_DETECTION_TYPE --test-class-name-regex $TEST_CLASS_REGEX -d . > testClasses"
              sf dxb source fetchtest -x $OUTPUT_DIR/package.xml -t $CHANGE_DETECTION_TYPE --test-class-name-regex $TEST_CLASS_REGEX -d . > testClasses    
              cat testClasses
            fi
            echo "noChanges=false" >> $GITHUB_OUTPUT

          fi
        shell: bash
        
        # 3. âš™ï¸ Set Specific Org Data (New Conditional Step)
      - name: âš™ï¸ Set Specific Org Data
        if: ${{ inputs.org-preconfig == true }}
        run: |
          TARGETENV= ${{ inputs.target-env }}
          # Check if the output of 'sfdx-env-mapping' is non-empty
          if [[ -s "${{ vars.ORG_SPEC_METADATA_DEF }}" ]]; then
            CONFIG_FILE="${{ vars.ORG_SPEC_METADATA_DEF }}"
            echo "Running: sf dxb org data --config $CONFIG_FILE --environment $TARGETENV"
            sf dxb org data --config "$CONFIG_FILE" --environment "$TARGETENV"
          else
            echo "Org. Spec Metadata Def file is empty or not found. Skipping 'Set Specific Org Data'. Go to Settings > Environment and add variable 'ORG_SPEC_METADATA_DEF' with the path of your file."
          fi
        shell: bash
        
      - name: ðŸ›¡ï¸ Run Code Analyzer
        id: code_analyzer
        continue-on-error: true # Azure's 'continueOnError: true'
        if: ${{ inputs.run-code-scanner == true }}
        run: |
          # apexclasses file was generated in the 'Calculate Delta' step.
          # It contains a comma-separated list of changed Apex class paths.
          APEX_CLASSES=$(cat apexclasses)
          
          # Check if the file content is NOT empty/just whitespace, similar to Azure's 'if [[ ! -z "$apexclasses" ]]'
          if [[ -s apexclasses ]]; then
            echo "--- Starting Code Scan on $APEX_CLASSES ---"
            
            # Create output directory
            mkdir -p codeanalyzeroutput
            
            # Run the scan. We use the SEVERITY_THRESHOLD env var defined earlier.
            sf code-analyzer run \
              -t "$APEX_CLASSES" \
              -f "codeanalyzeroutput/code-scanner-results.html" \
            
            # Set output variable to track if a report was generated (equivalent to Azure's task.setvariable)
            echo "codeScannerReportExists=true" >> $GITHUB_OUTPUT
          
          else
            echo "Nothing to scan (apexclasses file is empty)."
            echo "codeScannerReportExists=false" >> $GITHUB_OUTPUT
          fi
        shell: bash
      # ... (After the 'Calculate Delta' step, which prepares the package.xml)

      - name: ðŸš€ Salesforce Deployment
        # Use an 'if' condition to ensure this step only runs if noChanges is false.
        # We assume you set a step output or environment variable 'noChanges'
        # in your 'Calculate Delta' step.
        id: sfdeploy
        if: ${{ steps.calculate_delta.outputs.noChanges == 'false' }}
        run: |
          echo "runTests=false" >> $GITHUB_OUTPUT
          OPTIONS=" -g --wait 666 --json"
          predestructivepath="${{ vars.PRE_DESTRUCTIVE_XML_PATH }}"
          postdestructivepath="${{ vars.POST_DESTRUCTIVE_XML_PATH }}"
          
          # --- TEST CLASSES (Requires output from 'Calculate Delta' step if RunSpecifiedTests is used) ---
          # Since you didn't define testClasses globally, we assume it's calculated in the delta step.
          # For simplicity, we'll try to use the logic from your provided code:
          TESTCLASSES=""
          if [ "$TEST_LEVEL" = "RunSpecifiedTests" ]; then 
            # Assuming 'testClasses' file was created by 'Calculate Delta'
            if [ -f "testClasses" ]; then
              input_string=$(cat testClasses)
              IFS=',' read -ra values <<< "$input_string"
              for value in "${values[@]}"; do
                TESTCLASSES+="-t $value "
              done
            fi
          fi
          
          # -----------------------------------------------------------
          # 2. Re-apply Deployment Option Overrides
          # -----------------------------------------------------------
          
          if [ "$CHECKONLY" = "true" ]; then
              OPTIONS="$OPTIONS --dry-run"
          fi
          if [ -n "$predestructivepath" ] && [ -f "$predestructivepath" ]; then
              OPTIONS="$OPTIONS --pre-destructive-changes $predestructivepath"
          fi
          if [ -n "$postdestructivepath" ] && [ -f "$postdestructivepath" ]; then
              OPTIONS="$OPTIONS --post-destructive-changes $postdestructivepath"
          fi
          if [ "$TEST_LEVEL" = "RunSpecifiedTests" ] || [ "$TEST_LEVEL" = "RunLocalTests" ] ; then
              OPTIONS="$OPTIONS --junit --results-dir testresults/apex --coverage-formatters cobertura"
              echo "runTests=true" >> $GITHUB_OUTPUT
          fi
          
          echo "--- Initiating Deployment ---"
          echo "Test Level: $TEST_LEVEL"
          echo "Test Classes: $TESTCLASSES"
          echo "Deployment Options: $OPTIONS"
          
          # -----------------------------------------------------------
          # 3. Execute the SFDX Deployment Command
          # -----------------------------------------------------------
          echo "sf project start deploy -o ${{ inputs.target-env }} -x ${{ inputs.outputdir }}/package.xml -l $TEST_LEVEL $TESTCLASSES $OPTIONS" 
          # Use the target-env input, the package.xml from the delta step,
          # and all the calculated variables.
          
          # The command execution
          sf project start deploy \
            -o ${{ inputs.target-env }} \
            -x ${{ inputs.outputdir }}/package.xml \
            -l $TEST_LEVEL \
            $TESTCLASSES \
            $OPTIONS > deployResult.json
            
          # Output the result
          cat deployResult.json | jq 
      
        shell: bash

      - name: ðŸŽ¯ Deployment Result
        if: '!cancelled()'
        run: | 
          # Define the output file for the GitHub Job Summary
          SUMMARY_FILE=$GITHUB_STEP_SUMMARY
          DEPLOY_RESULT_JSON="deployResult.json"
          
          # --- 1. Extract Key Metrics from JSON ---
          
          # We use jq to safely extract values, defaulting to 0 or 'null' if paths don't exist
          JOBID=$(jq -r '.result.id // "N/A"' "$DEPLOY_RESULT_JSON")
          SUCCESS=$(jq -r '.result.success // "false"' "$DEPLOY_RESULT_JSON")
          FAILEDPREDEPLOY=$(jq -r '.exitCode // "0"' "$DEPLOY_RESULT_JSON")
          
          # Component Metrics (Available for any deployment)
          NUMBERCOMPONENTSTOTAL=$(jq -r '.result.numberComponentsTotal // "0"' "$DEPLOY_RESULT_JSON")
          NUMBERCOMPONENTERRORS=$(jq -r '.result.numberComponentErrors // "0"' "$DEPLOY_RESULT_JSON")
          
          # Test Metrics (Only available if tests were run)
          NUMTESTSRUN=$(jq -r '.result.details.runTestResult.numTestsRun // "0"' "$DEPLOY_RESULT_JSON")
          TESTNUMFAILURES=$(jq -r '.result.details.runTestResult.numFailures // "0"' "$DEPLOY_RESULT_JSON")
          
          # --- 2. Determine Overall Deployment Status ---
          
          STATUS_ICON="âšª UNKNOWN"
          STATUS_TITLE="Deployment Report"
          
          if [ "$JOBID" = "N/A" ] && [ "$FAILEDPREDEPLOY" != "0" ]; then
              STATUS_ICON="âŒ BLOCKED"
              STATUS_TITLE="Deployment Failed Before Start"
              DEPLOYMENT_MESSAGE="The deployment could not start due to a system error (e.g., login failure, invalid manifest)."
          elif [ "$SUCCESS" = "true" ]; then
              STATUS_ICON="âœ… SUCCESS"
              STATUS_TITLE="Deployment Succeeded"
              DEPLOYMENT_MESSAGE="All components and required tests passed successfully."
          elif [ "$SUCCESS" = "false" ]; then
              STATUS_ICON="ðŸ›‘ FAILED"
              STATUS_TITLE="Deployment Failed"
              
              if [ "$NUMBERCOMPONENTERRORS" != "0" ]; then
                  DEPLOYMENT_MESSAGE="Component deployment failed with **$NUMBERCOMPONENTERRORS** error(s) out of $NUMBERCOMPONENTSTOTAL components."
              elif [ "$TESTNUMFAILURES" != "0" ]; then
                  DEPLOYMENT_MESSAGE="Apex tests failed: **$TESTNUMFAILURES** failure(s) out of $NUMTESTSRUN tests."
              else
                  DEPLOYMENT_MESSAGE="Deployment failed for an unknown reason. Check raw JSON."
              fi
          fi
          
          # --- 3. Write Markdown Summary Header ---
          
          echo "# $STATUS_ICON $STATUS_TITLE" >> $SUMMARY_FILE
          echo "---" >> $SUMMARY_FILE
          echo "$DEPLOYMENT_MESSAGE" >> $SUMMARY_FILE
          echo "" >> $SUMMARY_FILE
          
          # --- 4. Write Metrics Table ---
          
          echo "### ðŸ“Š Core Metrics" >> $SUMMARY_FILE
          echo "| Metric | Value |" >> $SUMMARY_FILE
          echo "| :--- | :---: |" >> $SUMMARY_FILE
          echo "| Components Processed | **$NUMBERCOMPONENTSTOTAL** |" >> $SUMMARY_FILE
          echo "| Component Errors | **$NUMBERCOMPONENTERRORS** |" >> $SUMMARY_FILE
          echo "| Tests Run | **$NUMTESTSRUN** |" >> $SUMMARY_FILE
          echo "| Test Failures | **$TESTNUMFAILURES** |" >> $SUMMARY_FILE
          echo "| Salesforce Job ID | `$JOBID` |" >> $SUMMARY_FILE
          echo "" >> $SUMMARY_FILE
          
          # --- 5. Write Detailed Errors Section ---
          
          if [ "$SUCCESS" = "false" ] || [ "$FAILEDPREDEPLOY" != "0" ]; then
              echo "### ðŸš¨ Breakdown of Failures" >> $SUMMARY_FILE
              
              # A. Pre-Deployment Failure (e.g., invalid connection/manifest)
              if [ "$JOBID" = "N/A" ] && [ "$FAILEDPREDEPLOY" != "0" ]; then
                  echo "The deployment command exited with status code $FAILEDPREDEPLOY. Review the job logs for the full command output." >> $SUMMARY_FILE
                  
              # B. Component Failures
              elif [ "$NUMBERCOMPONENTERRORS" != "0" ]; then
                  # Use jq to format the component failures list nicely in a code block
                  echo "" >> $SUMMARY_FILE
                  echo "<details><summary>Component Errors Details ($NUMBERCOMPONENTERRORS)</summary>" >> $SUMMARY_FILE
                  echo "" >> $SUMMARY_FILE
                  echo "\`\`\`" >> $SUMMARY_FILE
                  jq -r '.result.details.componentFailures[] | "\(.fullName) (\(.componentType))\n Line \(.lineNumber): \(.problem)\n---"' "$DEPLOY_RESULT_JSON" >> $SUMMARY_FILE
                  echo "\`\`\`" >> $SUMMARY_FILE
                  echo "</details>" >> $SUMMARY_FILE
                  
              # C. Test Failures
              elif [ "$TESTNUMFAILURES" != "0" ]; then
                  # Use jq to format the test failures list nicely
                  echo "" >> $SUMMARY_FILE
                  echo "<details><summary>Apex Test Failures ($TESTNUMFAILURES)</summary>" >> $SUMMARY_FILE
                  echo "" >> $SUMMARY_FILE
                  echo "\`\`\`" >> $SUMMARY_FILE
                  jq -r '.result.details.runTestResult.failures[] | "Test: \(.name) (\(.methodName))\n Message: \(.message)\n Stack: \(.stackTrace)\n---"' "$DEPLOY_RESULT_JSON" >> $SUMMARY_FILE
                  echo "\`\`\`" >> $SUMMARY_FILE
                  echo "</details>" >> $SUMMARY_FILE
              fi
              
              echo "" >> $SUMMARY_FILE
              echo "**Action Required:** Deployment failed. Please review the detailed logs and fix the reported errors." >> $SUMMARY_FILE
              
          fi
          
          echo "---" >> $SUMMARY_FILE
          echo "[Download Raw deployResult.json Artifact]" >> $SUMMARY_FILE
        shell: bash
        
      - name: ðŸ§¹ Coverage Cleanup and Checks
        # Only run if tests were executed successfully (based on the previous step's output)
        if: ${{ steps.sfdeploy.outputs.runTests == 'true' }}
        run: |
          # Define file paths
          COVERAGE_FILE="testresults/apex/coverage/cobertura.xml"
          JUNIT_FILE="testresults/apex/junit/junit.xml"
          
          # Map optional inputs (assuming they exist in your workflow_call inputs)
          MINCODECOVERAGE="${{ vars.MIN_CODE_COVERAGE }}"
          CHECKPERFORMANCE="${{ vars.CHECKPERFORMANCE }}"
          
          # Map other required inputs
          CHECKONLY="${{ inputs.check-only }}"
          
          # Check if both report files exist before proceeding
          if [ -e "$COVERAGE_FILE" ] && [ -e "$JUNIT_FILE" ]; then
              
              echo "--- Starting Apex Report Cleanup ---"
              echo "sf dxb apex coverage cleanup -f $COVERAGE_FILE"
              echo "Update code coverage cobertura file to align apex class file path..."
              
              # Displaying the content BEFORE cleanup (for debugging/logs)
              # We limit the output to prevent log spam if the file is huge
              echo "--- Cobertura XML before cleanup (first 20 lines) ---"
              head -n 20 "$COVERAGE_FILE"
              echo "--------------------------------------------------------"
              
              # 1. Clean up the Cobertura XML file paths
              sf dxb apex coverage cleanup -f "$COVERAGE_FILE"
              
              # -------------------------------------------------------------
              # 2. Check Minimum Code Coverage
              # -------------------------------------------------------------
              
              # The coverage check should only be run if mincodecoverage is provided AND it's a validation run (Check Only)
              if [ -n "$MINCODECOVERAGE" ] && [ "$CHECKONLY" = "true" ]; then
                  echo "--- Verify Minimum Apex Code Coverage: $MINCODECOVERAGE% ---"
                  echo "sf dxb apex coverage check -f $COVERAGE_FILE -c $MINCODECOVERAGE"
                  
                  # This command will fail the step if coverage is too low
                  sf dxb apex coverage check -f "$COVERAGE_FILE" -c "$MINCODECOVERAGE"
              fi
              
              # -------------------------------------------------------------
              # 3. Check Performance (JUNIT)
              # -------------------------------------------------------------
              
              # The performance check runs if the input is provided
              if [ -n "$CHECKPERFORMANCE" ]; then
                  # Assuming CHECKPERFORMANCE provides the max time (e.g., "3" for 3 seconds)
                  MAX_TIME_SECONDS="$CHECKPERFORMANCE"
                  echo "--- Verify Test Performance (Max $MAX_TIME_SECONDS seconds) ---"
                  echo "sf dxb junit check -p $JUNIT_FILE -t $MAX_TIME_SECONDS"
                  
                  # This command will fail the step if any test takes longer than the threshold
                  sf dxb junit check -p "$JUNIT_FILE" -t "$MAX_TIME_SECONDS"
              fi
              
          else
              echo "Skipping coverage cleanup/checks: Required files ($COVERAGE_FILE or $JUNIT_FILE) not found."
          fi
        shell: bash

      - name: ðŸŽ¨Cobertura Summary
        # Ensure this step runs only if tests ran and produced the XML
        if: ${{ steps.sfdeploy.outputs.runTests == 'true' }}
        run: |
          # Define file paths and GitHub URLs
          COVERAGE_FILE="testresults/apex/coverage/cobertura.xml"
          BASE_FILE_URL="${{ vars.GH_BASE_URL }}"
      
          # -------------------------------------------------------------
          # 1. WRITE HEADER
          # -------------------------------------------------------------
          
          # Extract global coverage data for the header (as before)
          SUMMARY_DATA=$(grep -oP '<coverage [^>]*>' "$COVERAGE_FILE" | head -1)
          LINE_RATE=$(echo "$SUMMARY_DATA" | grep -oP 'line-rate="\K[^"]+')
          COVERAGE_PCT=$(awk "BEGIN {printf \"%.2f\n\", $LINE_RATE * 100}")
          
          # Write header
          echo "## ðŸŽ¯ File-Level Code Coverage Summary (${COVERAGE_PCT}%)" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          
          # Define the new table structure
          echo "| File | Line Coverage | ðŸ”´ Uncovered Lines |" >> $GITHUB_STEP_SUMMARY
          echo "| :--- | :---: | :--- |" >> $GITHUB_STEP_SUMMARY
          
          # -------------------------------------------------------------
          # 2. EXTRACT AND FORMAT FILE-LEVEL DETAIL (The complex part)
          # -------------------------------------------------------------
          
          # AWK script to process the XML:
          # 1. Finds the filename and line-rate from the <class> tag.
          # 2. Iterates through subsequent <line> tags to find 'hits="0"'.
          # 3. Prints one formatted Markdown row per file.
          
          awk -v BASE_URL="$BASE_FILE_URL" '
          # Initialize variables for each file block
          BEGIN { LINES_MISSED=""; FILENAME=""; CLASS_RATE=""; }
      
          # Pattern match for the end of a class block, and print the results
          /<class name=/ { 
              # Before processing the new class, check if the previous one had any missed lines
              if (FILENAME != "" && LINES_MISSED != "") {
                  # Format the output table row (only if missed lines exist)
                  printf "| [%s](%s) | **%s%%** | %s |\n", CLASS_NAME, FILE_HYPERLINK, CLASS_RATE_PCT, LINES_MISSED | "cat >> $GITHUB_STEP_SUMMARY"
              }
              
              # Reset variables for the new class
              LINES_MISSED="";
              
              # Extract filename (full relative path)
              match($0, /filename="[^"]+"/)
              FULL_PATH=substr($0, RSTART+10, RLENGTH-11)
              
              # Extract class name (for display)
              match($0, /name="[^"]+"/)
              CLASS_NAME=substr($0, RSTART+6, RLENGTH-7)
              
              # Extract class line-rate (for percentage column)
              match($0, /line-rate="[^"]+"/)
              CLASS_RATE=substr($0, RSTART+11, RLENGTH-12)
              
              # Calculate percentage and file link
              CLASS_RATE_PCT = sprintf("%.0f", CLASS_RATE * 100);
              FILE_HYPERLINK = BASE_URL FULL_PATH;
      
              # Store the current filename and rate for the next print cycle
              FILENAME=FULL_PATH;
          }
      
          # Pattern match for uncovered lines (hits="0")
          /<line / && /hits="0"/ {
              # Extract line number
              match($0, /number="[^"]+"/)
              LINENUM=substr($0, RSTART+8, RLENGTH-9)
              
              # Append line number to the list
              if (LINES_MISSED == "") {
                  LINES_MISSED = LINENUM;
              } else {
                  LINES_MISSED = LINES_MISSED ", " LINENUM;
              }
          }
      
          # End of file: process the very last class block
          END {
              if (FILENAME != "" && LINES_MISSED != "") {
                   printf "| [%s](%s) | **%s%%** | %s |\n", CLASS_NAME, FILE_HYPERLINK, CLASS_RATE_PCT, LINES_MISSED | "cat >> $GITHUB_STEP_SUMMARY"
              }
          }' "$COVERAGE_FILE"
      
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Files with 100% coverage are not listed in the detail section.*" >> $GITHUB_STEP_SUMMARY
          echo "[Download Full Cobertura XML Report Artifact]" >> $GITHUB_STEP_SUMMARY
        shell: bash
          
      - name: ðŸ“¤ Publish Code Analyzer Report
        uses: actions/upload-artifact@v4
        
        if: ${{ steps.code_analyzer.outputs.codeScannerReportExists == 'true' }}
        with:
          name: Code-Analyzer-Report-${{ inputs.target-env }}
          path: codeanalyzeroutput/code-scanner-results.html
          
          retention-days: 7
